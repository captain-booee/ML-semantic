{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Glove.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mlfd8zUEDajI"
      },
      "source": [
        "import ast\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB,GaussianNB\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import recall_score,precision_score\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.preprocessing import MaxAbsScaler \n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC  \n",
        "\n",
        "x = []\n",
        "y = []\n",
        "with open(\"drive/MyDrive/ml data/sets/data/train_glove.csv\", \"r\") as lines:\n",
        "  for i in lines.readlines()[1:]:\n",
        "    train_features = i.strip().split(\",\",2)\n",
        "    y.append(train_features[0])\n",
        "    x.append(ast.literal_eval(train_features[2].replace('\"','')))\n",
        "x_dev = []\n",
        "y_dev = []\n",
        "with open(\"drive/MyDrive/ml data/sets/data/dev_glove.csv\", \"r\") as lines:\n",
        "  for i in lines.readlines()[1:]:\n",
        "    dev_features = i.strip().split(\",\",2)\n",
        "    y_dev.append(dev_features[0])\n",
        "    x_dev.append(ast.literal_eval(dev_features[2].replace('\"','')))\n",
        "x_test = []\n",
        "tweet_id = []\n",
        "with open(\"drive/MyDrive/ml data/sets/data/test_glove.csv\", \"r\") as lines:\n",
        "  for i in lines.readlines()[1:]:\n",
        "    t_features = i.strip().split(\",\",2)\n",
        "    tweet_id.append(t_features[1])\n",
        "    x_test.append(ast.literal_eval(t_features[2].replace('\"','')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfSOq_WkIOdD"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler \n",
        "scaler = StandardScaler() \n",
        "scaler.fit(x)  \n",
        "X_train = scaler.transform(x) \n",
        "X_dev = scaler.transform(x_dev) \n",
        "\n",
        "logreg = LogisticRegression(solver='newton-cg').fit(X_train,y)\n",
        "y_pred_class_logistic = logreg.predict(X_dev)\n",
        "print(metrics.confusion_matrix(y_dev, y_pred_class_logistic,labels=[\"pos\", \"neg\", \"neu\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIthw5Y_IeFR"
      },
      "source": [
        "from sklearn.preprocessing import MaxAbsScaler \n",
        "#this makes 0 to one distribution\n",
        "scaler = MaxAbsScaler() \n",
        "scaler.fit(x)  \n",
        "X_train = scaler.transform(x) \n",
        "X_dev = scaler.transform(x_dev) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZcZRqPzIiAX"
      },
      "source": [
        "#mlp = MLPClassifier(alpha=0.50,max_iter=2000,solver='adam',hidden_layer_sizes=(1000),activation='tanh').fit(X_train, y)\n",
        "0.6718074952275696\n",
        "mlp = MLPClassifier(alpha=0.50,max_iter=2000,solver='adam',hidden_layer_sizes=(800),activation='tanh').fit(X_train, y)\n",
        "\n",
        "y_pred_class_perceptron = mlp.predict(x_dev)\n",
        "print(metrics.confusion_matrix(y_dev, y_pred_class_logistic,labels=[\"pos\", \"neg\", \"neu\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StJLXhWrIr6j"
      },
      "source": [
        "model = Perceptron(eta0=0.1,max_iter=200).fit(X_train, y)\n",
        "y_pred_class_perceptron = model.predict(X_dev)\n",
        "print(metrics.confusion_matrix(y_dev, y_pred_class_perceptron,labels=[\"pos\", \"neg\", \"neu\"]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgkQUYZqINaA"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm_classifier = SVC()\n",
        "svm_classifier.fit(x,y)\n",
        "svm_classifier.score(x_dev,y_dev)\n",
        "#it took 1.30 h and 0.6946649251481966 accuracy."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZq2jbrJJpWS"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "neigh = KNeighborsClassifier(n_neighbors=100)\n",
        "neigh.fit(x, y)\n",
        "knn_pred = neigh.predict(x_dev)\n",
        "print(metrics.accuracy_score(y_dev, knn_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCozCS_HJx9c"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=500)\n",
        "# increasing the trees can increase the accuracy bu time.....\n",
        "rf.fit(x, y)\n",
        "knn_pred = rf.predict(x_dev)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}